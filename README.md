Here is your README file content formatted for a GitHub repository:

---

# ğŸ§  LLM Medical Evaluation Framework

A comprehensive framework for evaluating multiple Large Language Models (LLMs) on medical question-answering tasks using the **PubMedQA** dataset. This project includes **data extraction**, **multi-metric evaluation**, and a **Streamlit dashboard** for interactive visualization and analysis.

---

## ğŸ“‹ Overview

This project provides a framework to:

* âœ… Extract medical questions and answers from the **PubMedQA** dataset
* ğŸ¤– Evaluate multiple **LLMs** on these questions using **RAGAS metrics**
* ğŸ“Š Visualize and compare performance through an **interactive dashboard**

---

## ğŸš€ Features

* **ğŸ§ª Multi-model Evaluation:**
  Compare the performance of:

  * GPT-4.1 Nano
  * LLaMA 3-8B
  * Nova Micro
  * Qwen 3-4B

* **ğŸ“ Multiple Evaluation Metrics:**

  * **Faithfulness:** Factual consistency with reference context
  * **Context Relevancy:** Relevance of context to the question
  * **Context Recall:** Coverage of important information from context

* **ğŸ“Š Interactive Dashboard:**

  * Streamlit-based
  * Model comparison
  * Visual plots and summary tables

---

## ğŸ› ï¸ Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/llm-medical-evaluation.git
cd llm-medical-evaluation

# Create and activate a virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

---

## ğŸ”§ Configuration

Create a `.env` file in the root directory:

```
NEXUS_API_KEY=your_nexus_api_key
NEXUS_API_URL=your_nexus_api_url
OPEN_ROUTER_API=your_open_router_api_key
```

---

## ğŸ“Š Usage

### 1. Download the Dataset

```bash
python data_extract.py
```

This downloads and prepares the **PubMedQA** dataset inside the `pubmed_dataset` directory.

---

### 2. Run Evaluations

You can run evaluations separately or comprehensively:

```bash
# Faithfulness metric
python faithfullness.py

# Context relevancy
python context_relevancy.py

# Context recall
python context_recall.py

# Or all at once
python evaluate.py
```

---

### 3. Launch the Dashboard

```bash
streamlit run dashboard.py
```

Access it at [http://localhost:8501](http://localhost:8501).

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ dashboard.py              # Streamlit dashboard
â”œâ”€â”€ data_extract.py           # Dataset extraction script
â”œâ”€â”€ evaluate.py               # Comprehensive evaluation runner
â”œâ”€â”€ faithfullness.py          # Faithfulness metric
â”œâ”€â”€ context_relevancy.py      # Context relevancy metric
â”œâ”€â”€ context_recall.py         # Context recall metric
â”œâ”€â”€ test_gpt3.py              # Test script for GPT-3 API
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ gpt_4_1_nano.py
â”‚   â”œâ”€â”€ llama8b.py
â”‚   â”œâ”€â”€ nova_micro.py
â”‚   â””â”€â”€ qwen.py
â”œâ”€â”€ pubmed_dataset/           # PubMedQA dataset
â”œâ”€â”€ result_ragas/             # Evaluation results
â”‚   â”œâ”€â”€ faithfulness_scores.csv
â”‚   â”œâ”€â”€ context_relevancy_scores.csv
â”‚   â””â”€â”€ context_recall_scores.csv
â””â”€â”€ ragas_evaluation/         # Visualizations
    â”œâ”€â”€ faithfulness_plot.png
    â”œâ”€â”€ context_relevancy_plot.png
    â””â”€â”€ context_recall_plot.png
```

---

## ğŸ“ˆ Dashboard Features

* ğŸ“‚ Tabs for each metric
* ğŸ§  Model selection
* ğŸ“Š Visual comparison
* ğŸ§® Summary table of all scores

---

## ğŸ” Evaluation Metrics

| Metric                | Description                                          |
| --------------------- | ---------------------------------------------------- |
| **Faithfulness**      | Checks factual alignment of answer with source       |
| **Context Relevancy** | Evaluates how relevant the context is                |
| **Context Recall**    | Measures how complete the answer is based on context |

---

## ğŸ§© Extending the Framework

### â• Add New Models

1. Create a new file in `models/`, e.g., `your_model.py`
2. Implement a `get_response(question, context)` function
3. Import and add it in `evaluate.py`

### â• Add New Metrics

1. Use new metrics from RAGAS or define your own
2. Update evaluation scripts
3. Modify `dashboard.py` to include visualizations

---

## ğŸ“ Requirements

* Python 3.8+
* Streamlit
* Pandas
* Matplotlib
* RAGAS
* LangChain
* Hugging Face Datasets
* OpenAI / compatible API access

---

## ğŸš§ Limitations & Roadmap

* Currently limited to **10 questions** for quick testing
* Planning to support:

  * More models
  * Advanced and custom metrics
  * Granular performance insights

---

## ğŸ“„ License

MIT License â€“ see the `LICENSE` file for details.

---

## ğŸ™ Acknowledgements

* [PubMedQA Dataset](https://pubmedqa.github.io/)
* [RAGAS](https://github.com/explodinggradients/ragas)
* [Streamlit](https://streamlit.io/)

---
